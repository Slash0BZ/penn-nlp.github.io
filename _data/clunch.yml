# The CLunch talks can be added to this file. They will appear on the website
# in the order that they appear here, so they should be listed in
# reverse-chronological order.
#
# Here is the template for an entry:
#
# - speaker: Name of the Speaker
#   url: http://speakers-website.com
#   affiliation: University Name
#   date: June 26, 2019
#   title: The talk of the title goes here.
#   abstract: The abstract of the talk will go here.

- speaker: Katharina Kann
  affiliation: New York University
  date: October 22, 2019
  title: Neural Networks for Morphological Generation in the Minimal-Resource Setting
  abstract: As languages other than English are moving more and more into the focus of natural language processing, accurate handling of morphology is increasing in importance. This talk presents neural network-based approaches to morphological generation, casting the problem as a character-based sequence-to-sequence task. First, we will generally discuss how to successfully train neural sequence-to-sequence models for this. Then, since many morphologically rich languages only have limited resources, the main part of the talk will focus on how to overcome the challenges that limited amounts of annotated training data pose to neural models. The approaches covered in this talk include multi-task learning, cross-lingual transfer learning, and meta-learning.

- speaker: Jithin Pradeep
  affiliation: The Vanguard Group
  date: October 15, 2019
  title: ArSI - Artificial Speech Intelligence - An end to end automatic speech recognition using Attention plus CTC

- speaker: Shi Yu
  affiliation: The Vanguard Group
  date: October 15, 2019
  title: A Financial Service Chatbot based on Deep Bidirectional Transformers

- speaker: Christopher Lynn
  url: https://web.sas.upenn.edu/chlynn/
  affiliation: University of Pennsylvania
  date: October 8, 2019
  title: Human information processing in complex networks
  abstract: Humans communicate using systems of interconnected stimuli or concepts -- from language and music to literature and science -- yet it remains unclear how, if at all, the structure of these networks supports the communication of information. Although information theory provides tools to quantify the information produced by a system, traditional metrics do not account for the inefficient and biased ways that humans process this information. Here we develop an analytical framework to study the information generated by a system as perceived by a human observer. We demonstrate experimentally that this perceived information depends critically on a system's network topology. Applying our framework to several real networks, we find that they communicate a large amount of information (having high entropy) and do so efficiently (maintaining low divergence from human expectations). Moreover, we show that such efficient communication arises in networks that are simultaneously heterogeneous, with high-degree hubs, and clustered, with tightly-connected modules -- the two defining features of hierarchical organization. Together, these results suggest that many real networks are constrained by the pressures of information transmission, and that these pressures select for specific structural features.

- speaker: Dan Goldwasser
  url: https://www.cs.purdue.edu/homes/dgoldwas/
  affiliation: Purdue University
  date: October 1, 2019
  title: Joint Models for Social, Behavioral and Textual Information
  abstract: Understanding natural language communication often requires context, such as the speakers' backgrounds and social conventions, however, when it comes to computationally modeling these interactions, we typically ignore their broader context and analyze the text in isolation. In this talk, I will review on-going work demonstrating the importance of holistically modeling behavioral, social and textual information. I will focus on several NLP problems, including political discourse analysis on Twitter, partisan news detection and open-domain debate stance prediction, and discuss how jointly modeling text and social behavior can help reduce the supervision effort and provide a better representation for language understanding tasks.

- speaker: Robert Shaffer
  url: https://global.upenn.edu/perryworldhouse/person/robert-shaffer
  affiliation: University of Pennsylvania
  date: September 24, 2019
  title: Similarity Inference for Legal Texts
  abstract: Quantifying similarity between pairs of documents is a ubiquitous task. Both researchers and members of the public frequently use document-level pairwise similarity measures to describe or explore unfamiliar corpora, or to test hypotheses regarding diffusion of ideas between authors. High-level similarity measures are particularly useful when dealing with legal or political corpora, which often contain long, thematically diverse, and specialized language that is difficult for non-experts to interpret. Unfortunately, though similarity estimation is a well-studied problem in the context of short documents and document excerpts, less attention has been paid to the problem of similarity inference for long documents.

- speaker: Reno Kriz
  url: https://rekriz11.github.io/
  affiliation: University of Pennsylvania
  date: September 17, 2019
  title: Comparison of Diverse Decoding Methods from Conditional Language Models
  abstract: While conditional language models have greatly improved in their ability to output high-quality natural language, many NLP applications benefit from being able to generate a diverse set of candidate sequences. Diverse decoding strategies aim to, within a given-sized candidate list, cover as much of the space of high-quality outputs as possible, leading to improvements for tasks that re-rank and combine candidate outputs. Standard decoding methods, such as beam search, optimize for generating high likelihood sequences rather than diverse ones, though recent work has focused on increasing diversity in these methods. We conduct an extensive survey of decoding-time strategies for generating diverse outputs from conditional language models. We also show how diversity can be improved without sacrificing quality by over-sampling additional candidates, then filtering to the desired number.

- speaker: Daphne Ippolito
  url: http://www.seas.upenn.edu/~daphnei/
  affiliation: University of Pennsylvania
  date: September 17, 2019
  title: Detecting whether Text is Human- or Machine-Generated
  abstract: With the advent of generative models with a billion parameters or more, it is now possible to automatically generate vast amounts of human-sounding text. But just how human-like is this machine-generated text? Intuitively, shorter amounts of machine-generated text are harder to detect, but exactly how many words can a machine generate and still fool both humans and trained discriminators? We investigate how the choices of sampling strategy and text sequence length impact discriminability from human-written text, using both automatic detection methods and human judgement.
